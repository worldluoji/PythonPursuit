# æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ä¸Žç¼“å­˜ç­–ç•¥

## âš¡ **æ•°æ®åº“æ€§èƒ½ç“¶é¢ˆåˆ†æžä¸Žä¼˜åŒ–**

---

### ðŸ” **æ€§èƒ½åˆ†æžå·¥å…·ä¸Žç›‘æŽ§**

```python
from sqlalchemy import event
from sqlalchemy.engine import Engine
import time
import logging

# é…ç½®SQLæŸ¥è¯¢æ—¥å¿—
logging.basicConfig()
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

class QueryPerformanceMonitor:
    """SQLæŸ¥è¯¢æ€§èƒ½ç›‘æŽ§å™¨"""
    
    def __init__(self):
        self.query_times = []
        self.slow_query_threshold = 1.0  # 1ç§’ä»¥ä¸Šè§†ä¸ºæ…¢æŸ¥è¯¢
    
    @staticmethod
    def setup_query_monitoring(engine: Engine):
        """è®¾ç½®æŸ¥è¯¢ç›‘æŽ§"""
        
        @event.listens_for(engine, "before_cursor_execute")
        def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            context._query_start_time = time.time()
        
        @event.listens_for(engine, "after_cursor_execute")
        def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            total_time = time.time() - context._query_start_time
            if total_time > 0.1:  # è®°å½•è¶…è¿‡100msçš„æŸ¥è¯¢
                print(f"â±ï¸ æ…¢æŸ¥è¯¢è­¦å‘Š: {total_time:.3f}s - {statement[:100]}...")
    
    def analyze_query_patterns(self, session):
        """åˆ†æžæŸ¥è¯¢æ¨¡å¼"""
        # æ¨¡æ‹Ÿåˆ†æžå¸¸è§æŸ¥è¯¢æ¨¡å¼
        common_patterns = [
            "N+1æŸ¥è¯¢é—®é¢˜æ£€æµ‹",
            "ç¼ºå°‘ç´¢å¼•çš„æŸ¥è¯¢",
            "å…¨è¡¨æ‰«ææŸ¥è¯¢",
            "è¿žæŽ¥æŸ¥è¯¢ä¼˜åŒ–å»ºè®®"
        ]
        
        print("ðŸ” æŸ¥è¯¢æ¨¡å¼åˆ†æž:")
        for pattern in common_patterns:
            print(f"  - {pattern}")

# åˆ›å»ºæ€§èƒ½ç›‘æŽ§å™¨
performance_monitor = QueryPerformanceMonitor()

def test_query_monitoring():
    """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½ç›‘æŽ§"""
    print("=== æŸ¥è¯¢æ€§èƒ½ç›‘æŽ§æµ‹è¯• ===")
    
    # è®¾ç½®ç›‘æŽ§
    performance_monitor.setup_query_monitoring(engine)
    
    with SessionLocal() as session:
        # æ¨¡æ‹Ÿä¸€äº›æŸ¥è¯¢
        users = session.query(User).limit(10).all()
        print(f"âœ… æŸ¥è¯¢åˆ° {len(users)} ä¸ªç”¨æˆ·")
        
        # åˆ†æžæŸ¥è¯¢æ¨¡å¼
        performance_monitor.analyze_query_patterns(session)

# test_query_monitoring()
```

### ðŸš€ **æŸ¥è¯¢ä¼˜åŒ–å®žæˆ˜æŠ€å·§**

```python
from sqlalchemy.orm import joinedload, selectinload, subqueryload

class OptimizedQueryExecutor:
    """ä¼˜åŒ–æŸ¥è¯¢æ‰§è¡Œå™¨"""
    
    def __init__(self, session):
        self.session = session
    
    def avoid_n_plus_1_problem(self):
        """é¿å…N+1æŸ¥è¯¢é—®é¢˜"""
        print("=== é¿å…N+1æŸ¥è¯¢é—®é¢˜ ===")
        
        # âŒ é”™è¯¯çš„åšæ³•ï¼šN+1æŸ¥è¯¢
        print("âŒ N+1æŸ¥è¯¢ç¤ºä¾‹:")
        users = self.session.query(User).limit(5).all()
        for user in users:
            # æ¯æ¬¡å¾ªçŽ¯éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢
            orders_count = len(user.orders)  # è¿™é‡Œä¼šäº§ç”ŸNæ¬¡é¢å¤–æŸ¥è¯¢
            print(f"  ðŸ‘¤ {user.username} æœ‰ {orders_count} ä¸ªè®¢å•")
        
        # âœ… æ­£ç¡®çš„åšæ³•ï¼šä½¿ç”¨é¢„åŠ è½½
        print("\nâœ… ä¼˜åŒ–åŽçš„æŸ¥è¯¢:")
        users_optimized = (self.session.query(User)
                          .options(joinedload(User.orders))
                          .limit(5).all())
        
        for user in users_optimized:
            # æ‰€æœ‰è®¢å•æ•°æ®å·²ç»åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢ä¸­åŠ è½½
            orders_count = len(user.orders)
            print(f"  ðŸ‘¤ {user.username} æœ‰ {orders_count} ä¸ªè®¢å•")
    
    def use_select_in_load_for_collections(self):
        """ä½¿ç”¨selectinloadåŠ è½½é›†åˆå…³ç³»"""
        print("\n=== ä½¿ç”¨selectinloadä¼˜åŒ– ===")
        
        # å¯¹äºŽä¸€å¯¹å¤šå…³ç³»ï¼Œselectinloadé€šå¸¸æ¯”joinedloadæ›´é«˜æ•ˆ
        users = (self.session.query(User)
                .options(selectinload(User.orders))
                .limit(10).all())
        
        print(f"âœ… ä½¿ç”¨selectinloadåŠ è½½ {len(users)} ä¸ªç”¨æˆ·åŠå…¶è®¢å•")
    
    def batch_operations_optimization(self):
        """æ‰¹é‡æ“ä½œä¼˜åŒ–"""
        print("\n=== æ‰¹é‡æ“ä½œä¼˜åŒ– ===")
        
        # âŒ ä½Žæ•ˆçš„æ‰¹é‡æ’å…¥
        def inefficient_bulk_insert(products_data):
            for product_data in products_data:
                product = Product(**product_data)
                self.session.add(product)
            self.session.commit()
        
        # âœ… é«˜æ•ˆçš„æ‰¹é‡æ’å…¥
        def efficient_bulk_insert(products_data):
            # ä½¿ç”¨bulk_insert_mappingsé¿å…å¯¹è±¡åˆ›å»ºå¼€é”€
            self.session.bulk_insert_mappings(Product, products_data)
            self.session.commit()
        
        # æµ‹è¯•æ•°æ®
        test_products = [
            {"name": f"Product_{i}", "price": i * 10.0, "stock_quantity": 100}
            for i in range(1, 101)
        ]
        
        start_time = time.time()
        efficient_bulk_insert(test_products)
        efficient_time = time.time() - start_time
        
        print(f"âœ… æ‰¹é‡æ’å…¥100ä¸ªäº§å“è€—æ—¶: {efficient_time:.3f}s")

def test_query_optimization():
    """æµ‹è¯•æŸ¥è¯¢ä¼˜åŒ–"""
    with SessionLocal() as session:
        optimizer = OptimizedQueryExecutor(session)
        optimizer.avoid_n_plus_1_problem()
        optimizer.use_select_in_load_for_collections()
        optimizer.batch_operations_optimization()

# test_query_optimization()
```

---

## ðŸ’¾ **å¤šçº§ç¼“å­˜ç­–ç•¥è®¾è®¡**

### **1. å†…å­˜ç¼“å­˜è£…é¥°å™¨**

```python
def cache_decorator(prefix: str, expire: int = 300):
    """é€šç”¨ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # å°è¯•ä»Žç¼“å­˜èŽ·å–
            cached_result = cache_client.get(cache_key)
            if cached_result is not None:
                print(f"ðŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_key}")
                return json.loads(cached_result)
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æžœ
            print(f"ðŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå‡½æ•°: {func.__name__}")
            result = func(*args, **kwargs)
            
            # ç¼“å­˜ç»“æžœ
            cache_client.set(cache_key, json.dumps(result), ex=expire)
            print(f"ðŸ’¾ ç»“æžœå·²ç¼“å­˜: {cache_key}")
            
            return result
        return wrapper
    return decorator

class CachedProductService:
    """å¸¦ç¼“å­˜çš„äº§å“æœåŠ¡"""
    
    @cache_decorator("product", expire=600)  # 10åˆ†é’Ÿç¼“å­˜
    def get_product_by_id(self, product_id: int) -> Optional[Dict]:
        """èŽ·å–äº§å“è¯¦æƒ… - å¸¦ç¼“å­˜"""
        with SessionLocal() as session:
            product = session.query(Product).filter(Product.id == product_id).first()
            if product:
                return {
                    'id': product.id,
                    'name': product.name,
                    'price': product.price,
                    'stock': product.stock_quantity
                }
            return None
    
    @cache_decorator("products", expire=300)  # 5åˆ†é’Ÿç¼“å­˜
    def get_products_by_category(self, category_id: int, page: int = 1, size: int = 20) -> List[Dict]:
        """èŽ·å–åˆ†ç±»äº§å“åˆ—è¡¨ - å¸¦ç¼“å­˜"""
        with SessionLocal() as session:
            offset = (page - 1) * size
            products = (session.query(Product)
                       .filter(Product.category_id == category_id)
                       .offset(offset).limit(size).all())
            
            return [
                {
                    'id': p.id,
                    'name': p.name,
                    'price': p.price,
                    'stock': p.stock_quantity
                }
                for p in products
            ]
    
    def invalidate_product_cache(self, product_id: int):
        """ä½¿äº§å“ç¼“å­˜å¤±æ•ˆ"""
        cache_keys = [
            f"product:get_product_by_id:{hash(str((product_id,)) + str({}))}",
            f"products:get_products_by_category:*"  # é€šé…ç¬¦å¤±æ•ˆï¼ˆå®žé™…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
        ]
        
        for key_pattern in cache_keys:
            # å®žé™…é¡¹ç›®ä¸­éœ€è¦ä½¿ç”¨Redisçš„keysæˆ–scanå‘½ä»¤
            print(f"ðŸ—‘ï¸ ä½¿ç¼“å­˜å¤±æ•ˆ: {key_pattern}")

def test_caching_strategy():
    """æµ‹è¯•ç¼“å­˜ç­–ç•¥"""
    print("=== ç¼“å­˜ç­–ç•¥æµ‹è¯• ===")
    
    service = CachedProductService()
    
    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    print("ðŸ”„ ç¬¬ä¸€æ¬¡æŸ¥è¯¢:")
    product1 = service.get_product_by_id(1)
    print(f"  ç»“æžœ: {product1}")
    
    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    print("\nðŸ”„ ç¬¬äºŒæ¬¡æŸ¥è¯¢:")
    product2 = service.get_product_by_id(1)
    print(f"  ç»“æžœ: {product2}")
    
    # ä½¿ç¼“å­˜å¤±æ•ˆ
    print("\nðŸ—‘ï¸ ä½¿ç¼“å­˜å¤±æ•ˆ:")
    service.invalidate_product_cache(1)
    
    # ç¬¬ä¸‰æ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜å¤±æ•ˆåŽé‡æ–°æŸ¥è¯¢ï¼‰
    print("\nðŸ”„ ç¬¬ä¸‰æ¬¡æŸ¥è¯¢:")
    product3 = service.get_product_by_id(1)
    print(f"  ç»“æžœ: {product3}")

# test_caching_strategy()
```

### **2. åˆ†å¸ƒå¼ç¼“å­˜ä¸Žæœ¬åœ°ç¼“å­˜ç»“åˆ**

```python
class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ï¼šæœ¬åœ°ç¼“å­˜ + åˆ†å¸ƒå¼ç¼“å­˜"""
    
    def __init__(self):
        self.local_cache = {}  # æœ¬åœ°å†…å­˜ç¼“å­˜
        self.redis_cache = cache_client  # åˆ†å¸ƒå¼ç¼“å­˜
        self.local_ttl = 60  # æœ¬åœ°ç¼“å­˜1åˆ†é’Ÿ
    
    def get(self, key: str) -> Optional[Any]:
        # ç¬¬ä¸€çº§ï¼šæœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            cached_data, timestamp = self.local_cache[key]
            if time.time() - timestamp < self.local_ttl:
                print(f"ðŸ’¾ æœ¬åœ°ç¼“å­˜å‘½ä¸­: {key}")
                return cached_data
            else:
                # æœ¬åœ°ç¼“å­˜è¿‡æœŸ
                del self.local_cache[key]
        
        # ç¬¬äºŒçº§ï¼šRedisç¼“å­˜
        redis_data = self.redis_cache.get(key)
        if redis_data is not None:
            print(f"ðŸ”— Redisç¼“å­˜å‘½ä¸­: {key}")
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.local_cache[key] = (json.loads(redis_data), time.time())
            return json.loads(redis_data)
        
        print(f"ðŸ”„ ç¼“å­˜æœªå‘½ä¸­: {key}")
        return None
    
    def set(self, key: str, value: Any, redis_expire: int = 300):
        """è®¾ç½®å¤šçº§ç¼“å­˜"""
        # è®¾ç½®æœ¬åœ°ç¼“å­˜
        self.local_cache[key] = (value, time.time())
        
        # è®¾ç½®Redisç¼“å­˜
        self.redis_cache.set(key, json.dumps(value), ex=redis_expire)
        print(f"ðŸ’¾ è®¾ç½®å¤šçº§ç¼“å­˜: {key}")
    
    def delete(self, key: str):
        """åˆ é™¤å¤šçº§ç¼“å­˜"""
        if key in self.local_cache:
            del self.local_cache[key]
        self.redis_cache.delete(key)
        print(f"ðŸ—‘ï¸ åˆ é™¤å¤šçº§ç¼“å­˜: {key}")

# æµ‹è¯•å¤šçº§ç¼“å­˜
def test_multi_level_cache():
    print("=== å¤šçº§ç¼“å­˜æµ‹è¯• ===")
    
    cache = MultiLevelCache()
    
    # è®¾ç½®ç¼“å­˜
    cache.set("user:1", {"name": "Alice", "email": "alice@example.com"})
    
    # ç¬¬ä¸€æ¬¡èŽ·å–ï¼ˆæœ¬åœ°ç¼“å­˜ï¼‰
    user1 = cache.get("user:1")
    print(f"ç¬¬ä¸€æ¬¡èŽ·å–: {user1}")
    
    # ç«‹å³ç¬¬äºŒæ¬¡èŽ·å–ï¼ˆæœ¬åœ°ç¼“å­˜å‘½ä¸­ï¼‰
    user2 = cache.get("user:1")
    print(f"ç¬¬äºŒæ¬¡èŽ·å–: {user2}")
    
    # åˆ é™¤ç¼“å­˜
    cache.delete("user:1")
    
    # ç¬¬ä¸‰æ¬¡èŽ·å–ï¼ˆç¼“å­˜å¤±æ•ˆï¼‰
    user3 = cache.get("user:1")
    print(f"ç¬¬ä¸‰æ¬¡èŽ·å–: {user3}")

# test_multi_level_cache()
```

---

## ðŸ“Š **æ•°æ®åº“è¿žæŽ¥æ± ä¼˜åŒ–**

```python
from sqlalchemy.pool import QueuePool, StaticPool
import threading
from concurrent.futures import ThreadPoolExecutor

class ConnectionPoolOptimizer:
    """æ•°æ®åº“è¿žæŽ¥æ± ä¼˜åŒ–"""
    
    def __init__(self):
        self.optimized_engine = None
    
    def create_optimized_pool(self, database_url: str, pool_size: int = 20, max_overflow: int = 30):
        """åˆ›å»ºä¼˜åŒ–çš„è¿žæŽ¥æ± """
        self.optimized_engine = create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=pool_size,        # è¿žæŽ¥æ± å¤§å°
            max_overflow=max_overflow,   # æœ€å¤§æº¢å‡ºè¿žæŽ¥æ•°
            pool_recycle=3600,          # è¿žæŽ¥å›žæ”¶æ—¶é—´ï¼ˆç§’ï¼‰
            pool_pre_ping=True,          # è¿žæŽ¥å‰pingæ£€æµ‹
            echo_pool=True              # æ‰“å°è¿žæŽ¥æ± äº‹ä»¶
        )
        return self.optimized_engine
    
    def test_concurrent_connections(self, num_threads: int = 50):
        """æµ‹è¯•å¹¶å‘è¿žæŽ¥æ€§èƒ½"""
        if not self.optimized_engine:
            raise ValueError("è¯·å…ˆåˆ›å»ºä¼˜åŒ–åŽçš„å¼•æ“Ž")
        
        SessionLocalOptimized = sessionmaker(bind=self.optimized_engine)
        
        def query_task(thread_id: int):
            """å¹¶å‘æŸ¥è¯¢ä»»åŠ¡"""
            try:
                with SessionLocalOptimized() as session:
                    # æ‰§è¡Œç®€å•æŸ¥è¯¢
                    user_count = session.query(User).count()
                    print(f"ðŸ§µ çº¿ç¨‹ {thread_id} æŸ¥è¯¢åˆ° {user_count} ä¸ªç”¨æˆ·")
                    return True
            except Exception as e:
                print(f"âŒ çº¿ç¨‹ {thread_id} é”™è¯¯: {e}")
                return False
        
        print(f"ðŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•: {num_threads} ä¸ªçº¿ç¨‹")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            results = list(executor.map(query_task, range(num_threads)))
        
        total_time = time.time() - start_time
        success_count = sum(results)
        
        print(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {success_count}/{num_threads} æˆåŠŸ, è€—æ—¶: {total_time:.2f}s")

def test_connection_pool():
    """æµ‹è¯•è¿žæŽ¥æ± ä¼˜åŒ–"""
    print("=== è¿žæŽ¥æ± ä¼˜åŒ–æµ‹è¯• ===")
    
    optimizer = ConnectionPoolOptimizer()
    optimized_engine = optimizer.create_optimized_pool("sqlite:///optimized.db")
    
    # åˆ›å»ºæµ‹è¯•è¡¨
    Base.metadata.create_all(bind=optimized_engine)
    
    # æµ‹è¯•å¹¶å‘æ€§èƒ½
    optimizer.test_concurrent_connections(num_threads=30)

# test_connection_pool()
```

---

## ðŸŽ¯ **æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•**

### **âœ… å¿…é¡»ä¼˜åŒ–çš„é¡¹ç›®**
- [ ] **N+1æŸ¥è¯¢é—®é¢˜** - ä½¿ç”¨`joinedload`æˆ–`selectinload`
- [ ] **é€‚å½“çš„ç´¢å¼•** - ä¸ºæŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
- [ ] **è¿žæŽ¥æ± é…ç½®** - æ ¹æ®å¹¶å‘é‡è°ƒæ•´æ± å¤§å°
- [ ] **æŸ¥è¯¢ç¼“å­˜** - å¯¹é¢‘ç¹æŸ¥è¯¢ç»“æžœè¿›è¡Œç¼“å­˜

### **ðŸ” é«˜çº§ä¼˜åŒ–é¡¹ç›®**
- [ ] **æ•°æ®åº“åˆ†ç‰‡** - æ°´å¹³åˆ†å‰²å¤§æ•°æ®è¡¨
- [ ] **è¯»å†™åˆ†ç¦»** - ä¸»ä»Žæ•°æ®åº“æž¶æž„
- [ ] **æŸ¥è¯¢ç»“æžœåˆ†é¡µ** - é¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§é‡æ•°æ®
- [ ] **å®šæœŸæ¸…ç†** - å½’æ¡£åŽ†å²æ•°æ®

