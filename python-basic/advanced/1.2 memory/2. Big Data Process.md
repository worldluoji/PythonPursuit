# Big Data Process
分块读取策略正是处理大数据的黄金法则。

多层次内存管理架构:​
```
数据处理管道：
┌─────────────────┐
│   流式输入层     │ ← 文件流、网络流分块读取
│  (Streaming)    │
└─────────────────┘
        ↓
┌─────────────────┐
│   内存控制层     │ ← 分块处理、内存限制
│  (Memory Ctrl)  │
└─────────────────┐
        ↓
┌─────────────────┐
│   处理引擎层     │ ← 并行处理、结果聚合
│  (Processing)   │
└─────────────────┘
        ↓  
┌─────────────────┐
│   输出管理层     │ ← 流式写入、结果存储
│  (Output Mgmt)  │
└─────────────────┘
```

---

## 文件流
是的，Python 3 提供了多种读取文件流的方式。以下是常用的方法：

## 1. 基本文件读取方式

### 一次性读取整个文件
```python
# 小文件推荐
with open('file.txt', 'r', encoding='utf-8') as f:
    content = f.read()
    print(content)
```

### 逐行读取（推荐用于大文件）
```python
# 方式一：直接迭代文件对象
with open('file.txt', 'r', encoding='utf-8') as f:
    for line in f:
        print(line.strip())  # strip() 去除换行符

# 方式二：使用 readline()
with open('file.txt', 'r', encoding='utf-8') as f:
    line = f.readline()
    while line:
        print(line.strip())
        line = f.readline()

# 方式三：读取所有行到列表
with open('file.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()
    for line in lines:
        print(line.strip())
```

## 2. 流式读取（处理大文件）

### 按指定大小读取
```python
# 每次读取指定字节数
with open('large_file.txt', 'r', encoding='utf-8') as f:
    chunk_size = 1024  # 每次读取 1KB
    while True:
        chunk = f.read(chunk_size)
        if not chunk:  # 读取完毕
            break
        # 处理数据块
        process_chunk(chunk)
```

### 使用生成器实现流式处理
```python
def read_in_chunks(file_path, chunk_size=1024):
    """生成器函数，分块读取文件"""
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            yield chunk

# 使用生成器
for chunk in read_in_chunks('large_file.txt', 4096):
    process_chunk(chunk)
```

## 3. 二进制文件流读取

```python
# 读取二进制文件（如图片、视频）
with open('image.jpg', 'rb') as f:
    while True:
        chunk = f.read(8192)  # 每次读取 8KB
        if not chunk:
            break
        # 处理二进制数据
        process_binary_chunk(chunk)
```

## 4. 使用 `io` 模块的高级流操作

```python
import io

# StringIO - 内存中的文本流
text_stream = io.StringIO()
text_stream.write("Hello, World!")
text_stream.seek(0)  # 回到开头
content = text_stream.read()
print(content)

# BytesIO - 内存中的二进制流
byte_stream = io.BytesIO()
byte_stream.write(b"Binary data")
byte_stream.seek(0)
binary_content = byte_stream.read()
```

## 5. 网络流读取示例

```python
import urllib.request
import io

# 从URL读取流数据
url = 'http://example.com/large_file.txt'
with urllib.request.urlopen(url) as response:
    # 将网络响应转换为文件流
    stream = io.TextIOWrapper(response, encoding='utf-8')
    
    # 流式读取
    for line in stream:
        process_line(line.strip())
```

## 6. 使用缓冲读取提高性能

```python
# 使用缓冲读取大文件
def buffered_read(file_path, buffer_size=8192):
    """带缓冲的流式读取"""
    with open(file_path, 'r', encoding='utf-8') as f:
        buffer = ""
        while True:
            chunk = f.read(buffer_size)
            if not chunk:
                if buffer:  # 处理剩余数据
                    yield buffer
                break
            
            buffer += chunk
            lines = buffer.split('\n')
            buffer = lines[-1]  # 保存不完整的行
            
            for line in lines[:-1]:
                yield line + '\n'

# 使用缓冲读取
for line in buffered_read('large_file.txt'):
    process_line(line.strip())
```

## 7. 实际应用示例：处理大型日志文件

```python
def process_large_log_file(log_file_path):
    """流式处理大型日志文件"""
    count = 0
    error_count = 0
    
    with open(log_file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            # 实时处理每一行
            if 'ERROR' in line:
                error_count += 1
                print(f"第 {line_num} 行发现错误: {line.strip()}")
            
            count += 1
            
            # 每处理10000行输出一次进度
            if line_num % 10000 == 0:
                print(f"已处理 {line_num} 行，发现 {error_count} 个错误")
    
    print(f"处理完成！总共 {count} 行，发现 {error_count} 个错误")

# 使用示例
process_large_log_file('server.log')
```

## 8. 性能优化技巧

```python
# 使用较大的缓冲区
with open('large_file.txt', 'r', encoding='utf-8', buffering=8192) as f:
    for line in f:
        process_line(line)

# 禁用行缓冲，使用自定义缓冲
with open('large_file.txt', 'r', encoding='utf-8', buffering=0) as f:
    # 手动控制缓冲
    data = f.read(4096)
```

## 主要优势：

1. **内存高效**：不会一次性加载整个文件到内存
2. **实时处理**：可以边读取边处理
3. **可中断性**：处理过程中可以随时中断
4. **适合大文件**：能够处理远大于内存的文件

选择哪种方式取决于你的具体需求：文件大小、处理逻辑和性能要求。